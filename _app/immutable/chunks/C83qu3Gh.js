import{t as a,a as o}from"./DOjAhL_S.js";import"./CKglk6zZ.js";import{n as s}from"./BijH41Pn.js";const i={title:"Hello World!!",date:"2025-06-07T00:00:00.000Z",tags:"general",headings:[{depth:2,value:"The journey begins",id:"the-journey-begins"},{depth:2,value:"The movement",id:"the-movement"},{depth:2,value:"The big jump",id:"the-big-jump"},{depth:2,value:"What’s next?",id:"whats-next"},{depth:2,value:"References",id:"references"}]},{title:d,date:c,tags:m,headings:p}=i;var n=a(`<p>Hi everyone, my name is Ben, and I’m from Vietnam. I’m currently working (at the time of writing this post) as a Software Engineer at a company based in Sydney, Australia. My specialization is in Data Engineering & Backend Development. I started writing as a way to consolidate my knowledge and also to expose my stupidity :D. Through that help me improve my skills.
Here is the story about my adventure!</p> <h2 id="the-journey-begins"><a href="#the-journey-begins">The journey begins</a></h2> <p>I graduated from university in 2020 with a Bachelor degree in Software Engineering. I had no goal or plan at that time, but by fate, unknown forces brought me to Data Industry. With recommendation from my mentor, I started my career as a Data Engineer at a startup company which built a marketing data platform.</p> <p>Spending almost 2 years here was worth it, I learned a lot about data engineering and big data technologies. As a startup company, we were required to be innovative and strict on the costs. Instead of using cloud services, everything was built from scratch and deployed on bare metal servers. There was no Docker or CI/CD, just the FileZilla for uploading latest build files, start/stop the services manually, repeat the same process on other servers.</p> <p>I had a change to learn and apply various open-source technologies, namely: Spark, Hive, Iceberg, HBase, Solr, Elastic Search, Trino, Hadoop Ecosystem (HDFS, Yarn).</p> <p>The same as other bare metal server management, we faced a lot of issues, from critical bugs in the code which cause the whole database die, to server dead by unknown reasons. The most ridiculous issue we faced was server performance degration because it had 4 RAM slots in different brands (same specs, just different brands). It was stressful but when looked back, every issue was a valuable experience that can’t be found anywhere else.</p> <p>The biggest achievement I had done during this time was that I was able to use and customize Open Source projects to solve company’s problems. Some notable projects are implementing the Spark-Phoenix Connector for Spark 3.0+, which cut down 50% of ETL time compared to previous version, or the HBase indexer that listen the changes from HBase and index the documents into Elastic Search (aka Change Data Capture pipeline)</p> <h2 id="the-movement"><a href="#the-movement">The movement</a></h2> <p>The only thing that was lacking at my first company was the lack of operational process which is understandable as the company was a startup with only about 30 members at that time. So I took a gap and then moved to the next company, where I had a change to work for a big name in quantitative asset management (via a staffing arrangement).</p> <p>It was a new challenge that instead of dealing with technical issues, most of the problems were about clarification, process and human. Some problems took weeks to solve simply because we lived in different time zones (when I wake up, the other person is still sleeping and vice versa) and it took time to send emails back and forth. It was a really good experience though, I learned a lot about professional practices - from communication skills to tasks management. I got a feel about the how big company operates and how it manages its resources.</p> <p>Besides the soft skills, I also had a change to practice my technical skills on various “super big” datasets. Some datasets’ content was pretty sensitive - out of my imagination, some’s format was stupidly annoying - I saw 1 dataset which were CSV files but used custom encodings like ’@%$@%#*&’ for field and line separators because of security, so it was really hard to debug these files. It was really impressive to me that most of open source projects they used were forked and maybe customized to suit their needs - for example, the Spark’s worker was customized by using Pandas under the hood to support more file formats. There were some in-house technologies built from scratch and it looks like reinventing the wheel to me, but this is understandable as the company needs stability and risk management to ensure the reliability of their systems.</p> <p>The only things that were unsatisfying to me were, first, everything was changing slowly as expected from a big company, and second, a lot of the projects were in-house that would keep me out of the market in terms of technical skills. Therefore, I decided to transition into a remote job that can save a lot of my daily time, I used to spend hours to commute to work, it was really exhausting to be stuck in the middle of traffic jams and breathe the vehicle smoke everyday (we drive motorbike in my country), and helped me think more about my career goals.</p> <h2 id="the-big-jump"><a href="#the-big-jump">The big jump</a></h2> <p>During the time I worked, the biggest limitation of myself was a lack of theoretical and academic knowledge in data industry. I realized this when I failed a SE position (just the entry one) at Microsoft that when the interviewers asked me about the data quality issues I have faced in my experience, I could give an example but I couldn’t use the correct term to describe it (e.g data completeness, consistency, accuracy,…).
It’s good to have practices, but it would be much better if we have theories come along. Therefore, I have moved to Australia to pursue a higher degree in data science that it would help me to solidify my knowledge and skills in the data engineering field.</p> <h2 id="whats-next"><a href="#whats-next">What’s next?</a></h2> <p>This is a good time for me to practice and enhance my skills, so I started this website as my portfolio and a place for me to write and share my knowledge.
The topics are various, some could be:</p> <ul><li>Data & AI technologies: Yes, of course, this is my main major.</li> <li>System Design & CI/CD: Develop a service is easy but how can we efficiently deploy it? How to make it bring value to the business with the least cost? What is the tradeoff needed to consider?</li> <li>Gaming and real-time systems: First, because I love gaming. Second, because it’s similar to a highly optimized data/AI system - we collect and dispatch data but in a high throughput and low latency manner, also we process data in parallel using GPU - same as AI training. It would be interesting to practice and resolve the challenges in the field which are good experiences in system design.</li></ul> <p>Writing is a good way for me to learn and organize my knowledge. If you find anything interesting or have any suggestions, please feel free to contact me.</p> <p>Happy coding!</p> <h2 id="references"><a href="#references">References</a></h2> <ol><li>Thanks <a href="https://mattjennings.io/" rel="nofollow">@mattjennings</a> for the good website <a href="https://github.com/mattjennings/sveltekit-blog-template" rel="nofollow">template</a>.<br> The template was written in Svelte 3, but I ported it to Svelte 5.</li> <li>The tools/technologies mentioned:<br> <a href="https://spark.apache.org/" rel="nofollow">Spark,</a> <a href="https://hive.apache.org/" rel="nofollow">Hive,</a> <a href="https://iceberg.apache.org/" rel="nofollow">Iceberg,</a> <a href="https://hbase.apache.org/" rel="nofollow">HBase,</a> <a href="https://solr.apache.org/" rel="nofollow">Solr,</a><br> <a href="https://www.elastic.co/elasticsearch" rel="nofollow">Elastic Search,</a> <a href="https://trino.io/" rel="nofollow">Trino,</a> <a href="https://hadoop.apache.org/" rel="nofollow">Hadoop Ecosystem,</a><br> <a href="https://github.com/lucidworks/hbase-indexer" rel="nofollow">HBase indexer,</a> <a href="https://www.docker.com/" rel="nofollow">Docker,</a> <a href="https://filezilla-project.org/" rel="nofollow">FileZilla</a></li> <li>My personal project mentioned: <a href="https://github.com/anhvdq/phoenix-spark-connectors" rel="nofollow">Spark-Phoenix Connector</a></li></ol>`,1);function u(e){var t=n();s(40),o(e,t)}export{u as default,i as metadata};
